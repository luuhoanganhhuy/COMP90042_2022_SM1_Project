{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a35d77f",
   "metadata": {
    "id": "9a35d77f"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f974c5",
   "metadata": {
    "id": "27f974c5"
   },
   "outputs": [],
   "source": [
    "# read data \n",
    "train_data_file = open('/train.data.txt', 'r')\n",
    "train_lines = train_data_file.readlines()\n",
    "train_events =[]\n",
    "# Strips the newline character\n",
    "for line in train_lines:\n",
    "    train_events.append(list(map(int,line.strip('\\n').split(','))))\n",
    "    \n",
    "train_label_file = open('/train.label.txt', 'r') \n",
    "train_labels = train_label_file.readlines()\n",
    "train_labels = [label.strip('\\n') for label in train_labels]\n",
    "\n",
    "\n",
    "dev_data_file = open('/dev.data.txt', 'r')\n",
    "dev_lines = dev_data_file.readlines()\n",
    "dev_events =[]\n",
    "# Strips the newline character\n",
    "for line in dev_lines:\n",
    "    dev_events.append(list(map(int,line.strip('\\n').split(','))))\n",
    "    \n",
    "dev_label_file = open('/dev.label.txt', 'r')\n",
    "dev_labels = dev_label_file.readlines()\n",
    "dev_labels = [label.strip('\\n') for label in dev_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4e3125",
   "metadata": {
    "id": "3e4e3125"
   },
   "outputs": [],
   "source": [
    "# config to access tweeter API\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "consumer_key = config['twitter']['consumer_key']\n",
    "consumer_secret = config['twitter']['consumer_secret']\n",
    "\n",
    "access_token = config['twitter']['access_token']\n",
    "access_token_secret = config['twitter']['access_token_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac927c",
   "metadata": {
    "id": "50ac927c"
   },
   "outputs": [],
   "source": [
    "# authentication\n",
    "client = tweepy.Client(consumer_key=consumer_key, consumer_secret=consumer_secret,\n",
    "                                   access_token=access_token, access_token_secret=access_token_secret,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd867de",
   "metadata": {
    "id": "fdd867de"
   },
   "outputs": [],
   "source": [
    "# get_tweets only return 100 results, handle the case when there is more than 100\n",
    "def lookup_tweets(tweet_IDs, client):\n",
    "    full_tweets = []\n",
    "    tweet_count = len(tweet_IDs)\n",
    "    for i in range(int((tweet_count / 100) + 1)):\n",
    "        # Catch the last group if it is less than 100 tweets\n",
    "        end_loc = min((i + 1) * 100, tweet_count)\n",
    "        if tweet_IDs[i * 100:end_loc]:\n",
    "            tweets = client.get_tweets(tweet_IDs[i * 100:end_loc],user_auth=True).data\n",
    "            if tweets:\n",
    "                full_tweets.extend(tweets)\n",
    "    return full_tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc875eec",
   "metadata": {
    "id": "bc875eec",
    "outputId": "a436dcc4-57d9-40df-ef2a-4801cfe9d2d7"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17744/1230150304.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_events_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_events\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlookup_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_event_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtrain_events_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_event_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17744/631739775.py\u001b[0m in \u001b[0;36mlookup_tweets\u001b[1;34m(tweet_IDs, client)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mend_loc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtweet_IDs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_loc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_IDs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_loc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_auth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0mfull_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\site-packages\\tweepy\\client.py\u001b[0m in \u001b[0;36mget_tweets\u001b[1;34m(self, ids, user_auth, **params)\u001b[0m\n\u001b[0;32m   1701\u001b[0m                 \u001b[1;34m\"ids\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"expansions\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"media.fields\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"place.fields\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1702\u001b[0m                 \u001b[1;34m\"poll.fields\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tweet.fields\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"user.fields\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1703\u001b[1;33m             ), data_type=Tweet, user_auth=user_auth\n\u001b[0m\u001b[0;32m   1704\u001b[0m         )\n\u001b[0;32m   1705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\site-packages\\tweepy\\client.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         response = self.request(method, route, params=request_params,\n\u001b[1;32m--> 127\u001b[1;33m                                 json=json, user_auth=user_auth)\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_type\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\site-packages\\tweepy\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m     83\u001b[0m         with self.session.request(\n\u001b[0;32m     84\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhost\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mroute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         ) as response:\n\u001b[0;32m     87\u001b[0m             log.debug(\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    527\u001b[0m         }\n\u001b[0;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m                 )\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m             )\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1373\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1374\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1069\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1071\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\CV\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get the text of all events\n",
    "train_events_text=[]\n",
    "for event in train_events:\n",
    "    results = lookup_tweets(event, client)\n",
    "    train_event_text=[tweet.text for tweet in results]\n",
    "    train_events_text.append(train_event_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab73d8d",
   "metadata": {
    "id": "9ab73d8d"
   },
   "outputs": [],
   "source": [
    "# save data to pickle file\n",
    "f = open(f'./tweet_text.pckl','wb')\n",
    "pickle.dump(train_events_text,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9523f4",
   "metadata": {
    "id": "6a9523f4"
   },
   "outputs": [],
   "source": [
    "# get the text of all events\n",
    "dev_events_text=[]\n",
    "for event in dev_events:\n",
    "    results = lookup_tweets(event, client)\n",
    "    dev_event_text=[tweet.text for tweet in results]\n",
    "    dev_events_text.append(dev_event_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e26d0",
   "metadata": {
    "id": "140e26d0"
   },
   "outputs": [],
   "source": [
    "# save data to pickle file\n",
    "f = open(f'./dev_tweet_text.pckl','wb')\n",
    "pickle.dump(dev_events_text,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce426aaf",
   "metadata": {
    "id": "ce426aaf"
   },
   "outputs": [],
   "source": [
    "# open train text file\n",
    "f = open(f'/tweet_text.pckl','rb')\n",
    "train_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# open dev text file\n",
    "f = open(f'/dev_tweet_text.pckl','rb')\n",
    "dev_data = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f8d602",
   "metadata": {
    "id": "45f8d602"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text) #remove @mention\n",
    "    text = re.sub(r'#','',text) # remove the hashtag symbol\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '',text) #remove hyperlink\n",
    "    text = re.sub(r'\\n','',text) # remove \\n \n",
    "    text = re.sub(r'\\r','',text) # remove \\r\n",
    "    text = re.sub(r'[0-9]+','',text) #remove all the number\n",
    "    return text\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    for j in range(len(train_data[i])):\n",
    "        train_data[i][j] = clean_text(train_data[i][j])\n",
    "        \n",
    "for i in range(len(dev_data)):\n",
    "    for j in range(len(dev_data[i])):\n",
    "        dev_data[i][j] = clean_text(dev_data[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "IcTJP1AN1W8N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcTJP1AN1W8N",
    "outputId": "15cfefea-8907-4d2f-aa75-e6659ca11de3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. Can regularly rinsing your nose with saline help prevent infection with the new coronavirus? ',\n",
       " '. Can eating garlic help prevent infection with the new coronavirus? COVIDMalaysia ',\n",
       " '. Do vaccines against pneumonia protect you against the new coronavirus? ',\n",
       " '. Can spraying alcohol or chlorine all over your body kill the new coronavirus? Chamber ',\n",
       " '. How effective are thermal scanners in detecting people infected with the new coronavirus? ',\n",
       " '. Can an ultraviolet disinfection lamp kill the new coronavirus? ',\n",
       " '. Are hand dryers effective in killing the new coronavirus? ',\n",
       " '. The new coronavirus CANNOT be transmitted through mosquito bites. ',\n",
       " '. Taking a hot bath does not prevent the new coronavirus disease ',\n",
       " '. Cold weather and snow CANNOT kill the new coronavirus. ',\n",
       " '. COVID- virus can be transmitted in areas with hot and humid climates ',\n",
       " '. Drinking alcohol does not protect you against COVID- and can be dangerous ',\n",
       " '. Being able to hold your breath for  seconds or more without coughing or feeling discomfort DOES NOT mean you are free from the coronavirus disease (COVID-) or any other lung disease. ',\n",
       " '. You can recover from the coronavirus disease (COVID-). Catching the new coronavirus DOES NOT mean you will have it for life. ',\n",
       " '. Exposing yourself to the sun or to temperatures higher than C degrees DOES NOT prevent the coronavirus disease (COVID-) ',\n",
       " '. G mobile networks DO NOT spread COVID- ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6f359c",
   "metadata": {
    "id": "dd6f359c"
   },
   "outputs": [],
   "source": [
    "# merge source tweeet and reply tweet together for train data\n",
    "train_merge_events=[]\n",
    "for event in train_data:\n",
    "    merge = ''\n",
    "    for tweet in event:\n",
    "        merge = merge + tweet\n",
    "    train_merge_events.append(merge)\n",
    "    \n",
    "    \n",
    "# merge source tweeet and reply tweet together for dev data\n",
    "dev_merge_events=[]\n",
    "for event in dev_data:\n",
    "    merge = ''\n",
    "    for tweet in event:\n",
    "        merge = merge + tweet\n",
    "    dev_merge_events.append(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b3707",
   "metadata": {
    "id": "ef4b3707"
   },
   "outputs": [],
   "source": [
    "def tokenize_tweet(tweet):\n",
    "    \"\"\"Get all of the tokens in a set of tweets\"\"\"\n",
    "    twt = nltk.tokenize.TweetTokenizer()\n",
    "    # combine stop words and punctuation\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stop = stopwords + list(string.punctuation)\n",
    "    # filter out stop words and punctuation and send to lower case\n",
    "    tokens = [token.lower() for token in twt.tokenize(tweet)\n",
    "              if token.lower() not in stop]\n",
    "    tokens = [word for word in tokens if re.search('[a-zA-Z]',word) is not None] # filter out word not contain alphabet\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32163e",
   "metadata": {
    "id": "af32163e"
   },
   "outputs": [],
   "source": [
    "def tokenize_tweetv2(tweet):\n",
    "    \"\"\"Get all of the tokens in a set of tweets\"\"\"\n",
    "    twt = nltk.tokenize.TweetTokenizer()\n",
    "    # combine stop words and punctuation\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stop = stopwords + list(string.punctuation)\n",
    "    # create the stemmer\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    # filter out stop words and punctuation and send to lower case\n",
    "    tokens = [ stemmer.stem(token) for token in twt.tokenize(tweet)\n",
    "              if token.lower() not in stop]\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9d6b0",
   "metadata": {
    "id": "48e9d6b0"
   },
   "source": [
    "### Normal bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6e777",
   "metadata": {
    "id": "eeb6e777"
   },
   "outputs": [],
   "source": [
    "# Create bag of word \n",
    "def bow(data,labels):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)):\n",
    "        tokens = tokenize_tweet(data[i])\n",
    "        \n",
    "        vocab = collections.defaultdict(int)\n",
    "        for word in tokens:\n",
    "            vocab[word] += 1 \n",
    "        x.append(vocab)\n",
    "        y.append(labels[i])\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f2cb5",
   "metadata": {
    "id": "1d5f2cb5"
   },
   "outputs": [],
   "source": [
    "x_train,y_train = bow(train_merge_events,train_labels)\n",
    "x_dev,y_dev = bow(dev_merge_events,dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0e163",
   "metadata": {
    "id": "b6e0e163"
   },
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_dev = vectorizer.transform(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d51de",
   "metadata": {
    "id": "1f1d51de",
    "outputId": "ef223b8d-daaf-4b80-92f5-04f3497910ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With alpha = 0.001 the accuracy of Naive Bayes is 0.90032\n",
      "With alpha = 0.005 the accuracy of Naive Bayes is 0.89715\n",
      "With alpha = 0.01 the accuracy of Naive Bayes is 0.88924\n",
      "With alpha = 0.1 the accuracy of Naive Bayes is 0.88608\n",
      "With alpha = 0.3 the accuracy of Naive Bayes is 0.87816\n",
      "With alpha = 0.5 the accuracy of Naive Bayes is 0.88133\n",
      "With alpha = 1 the accuracy of Naive Bayes is 0.88608\n",
      "The best setting for Naive Bayes is alpha = 0.001 with accuracy = 0.90032\n"
     ]
    }
   ],
   "source": [
    "# k fold to find the optimize hyperparameter\n",
    "alphas = [0.001,0.005,0.01,0.1,0.3,0.5,1]\n",
    "max_nb = 0\n",
    "for alpha in alphas:\n",
    "    nb = MultinomialNB(alpha=alpha)\n",
    "    nb_predict = nb.fit(x_train, y_train).predict(x_dev)    \n",
    "    nb_accuracy = accuracy_score(y_dev,nb_predict)\n",
    "    print('With alpha = {alpha} the accuracy of Naive Bayes is {acc:.5f}'.format(alpha=alpha, acc = nb_accuracy))\n",
    "    if nb_accuracy > max_nb:\n",
    "        max_nb = nb_accuracy\n",
    "        max_alpha = alpha\n",
    "print(\"The best setting for Naive Bayes is alpha = {alpha} with accuracy = {acc:.5f}\".format(alpha=max_alpha,acc=max_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7355f",
   "metadata": {
    "id": "3fb7355f",
    "outputId": "383b2f11-fd96-420d-ee0e-d703ebbdb1cd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using this solver  newton-cg\n",
      "With C = 100 and solver  = newton-cg the acciracy of Logistic Regression is 0.9113924050632911\n",
      "With C = 10 and solver  = newton-cg the acciracy of Logistic Regression is 0.9145569620253164\n",
      "With C = 1.0 and solver  = newton-cg the acciracy of Logistic Regression is 0.9129746835443038\n",
      "With C = 0.1 and solver  = newton-cg the acciracy of Logistic Regression is 0.8876582278481012\n",
      "With C = 0.01 and solver  = newton-cg the acciracy of Logistic Regression is 0.8433544303797469\n",
      "With C = 0.001 and solver  = newton-cg the acciracy of Logistic Regression is 0.7958860759493671\n",
      "Using this solver  lbfgs\n",
      "With C = 100 and solver  = lbfgs the acciracy of Logistic Regression is 0.9113924050632911\n",
      "With C = 10 and solver  = lbfgs the acciracy of Logistic Regression is 0.9145569620253164\n",
      "With C = 1.0 and solver  = lbfgs the acciracy of Logistic Regression is 0.9129746835443038\n",
      "With C = 0.1 and solver  = lbfgs the acciracy of Logistic Regression is 0.8876582278481012\n",
      "With C = 0.01 and solver  = lbfgs the acciracy of Logistic Regression is 0.8433544303797469\n",
      "With C = 0.001 and solver  = lbfgs the acciracy of Logistic Regression is 0.7958860759493671\n",
      "Using this solver  liblinear\n",
      "With C = 100 and solver  = liblinear the acciracy of Logistic Regression is 0.9113924050632911\n",
      "With C = 10 and solver  = liblinear the acciracy of Logistic Regression is 0.9129746835443038\n",
      "With C = 1.0 and solver  = liblinear the acciracy of Logistic Regression is 0.9129746835443038\n",
      "With C = 0.1 and solver  = liblinear the acciracy of Logistic Regression is 0.8987341772151899\n",
      "With C = 0.01 and solver  = liblinear the acciracy of Logistic Regression is 0.8544303797468354\n",
      "With C = 0.001 and solver  = liblinear the acciracy of Logistic Regression is 0.8306962025316456\n",
      "Using this solver  sag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With C = 100 and solver  = sag the acciracy of Logistic Regression is 0.9272151898734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With C = 10 and solver  = sag the acciracy of Logistic Regression is 0.9272151898734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With C = 1.0 and solver  = sag the acciracy of Logistic Regression is 0.9272151898734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With C = 0.1 and solver  = sag the acciracy of Logistic Regression is 0.9224683544303798\n",
      "With C = 0.01 and solver  = sag the acciracy of Logistic Regression is 0.9145569620253164\n",
      "With C = 0.001 and solver  = sag the acciracy of Logistic Regression is 0.8781645569620253\n",
      "Using this solver  saga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With C = 100 and solver  = saga the acciracy of Logistic Regression is 0.9272151898734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With C = 10 and solver  = saga the acciracy of Logistic Regression is 0.9272151898734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With C = 1.0 and solver  = saga the acciracy of Logistic Regression is 0.9272151898734177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With C = 0.1 and solver  = saga the acciracy of Logistic Regression is 0.9240506329113924\n",
      "With C = 0.01 and solver  = saga the acciracy of Logistic Regression is 0.9145569620253164\n",
      "With C = 0.001 and solver  = saga the acciracy of Logistic Regression is 0.8718354430379747\n",
      "The best setting for Logistic Regression is c = 100 and solver = sag with accuracy = 0.92722\n"
     ]
    }
   ],
   "source": [
    "solvers = ['newton-cg', 'lbfgs', 'liblinear','sag','saga']\n",
    "c_values = [ 100,10,1.0, 0.1, 0.01,0.001]\n",
    "max_lr = 0\n",
    "for solver in solvers:\n",
    "    print('Using this solver ',solver )\n",
    "    for c_value in c_values:\n",
    "        lr = LogisticRegression(C=c_value, penalty='l2', solver=solver,max_iter=1000)\n",
    "        lr_predict = lr.fit(x_train, y_train).predict(x_dev)    \n",
    "        lr_accuracy = accuracy_score(y_dev,lr_predict)\n",
    "        print('With C = {c} and solver  = {sol} the acciracy of Logistic Regression is {acc}'.format(c=c_value,sol=solver,acc= lr_accuracy))\n",
    "        if lr_accuracy > max_lr:\n",
    "            max_lr = lr_accuracy\n",
    "            max_c_value = c_value\n",
    "            max_solver = solver\n",
    "print(\"The best setting for Logistic Regression is c = {c} and solver = {sol} with accuracy = {acc:.5f}\".format(c=max_c_value,sol=max_solver,acc=max_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4079c",
   "metadata": {
    "id": "e2d4079c"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clfs = [KNeighborsClassifier(),DecisionTreeClassifier(),RandomForestClassifier(),\n",
    "        MultinomialNB(),LinearSVC(),LogisticRegression()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23fdf92",
   "metadata": {
    "id": "e23fdf92",
    "outputId": "0f83f201-3061-447a-c0ea-9964a9c68a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier()\n",
      "accuracy\n",
      "0.7915567282321899\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.79      1.00      0.88      1475\n",
      "      rumour       0.86      0.07      0.13       420\n",
      "\n",
      "    accuracy                           0.79      1895\n",
      "   macro avg       0.82      0.53      0.51      1895\n",
      "weighted avg       0.81      0.79      0.72      1895\n",
      "\n",
      "DecisionTreeClassifier()\n",
      "accuracy\n",
      "0.833245382585752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.88      0.91      0.89      1475\n",
      "      rumour       0.64      0.56      0.60       420\n",
      "\n",
      "    accuracy                           0.83      1895\n",
      "   macro avg       0.76      0.73      0.75      1895\n",
      "weighted avg       0.83      0.83      0.83      1895\n",
      "\n",
      "RandomForestClassifier()\n",
      "accuracy\n",
      "0.8401055408970977\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.83      0.99      0.91      1475\n",
      "      rumour       0.93      0.30      0.45       420\n",
      "\n",
      "    accuracy                           0.84      1895\n",
      "   macro avg       0.88      0.65      0.68      1895\n",
      "weighted avg       0.86      0.84      0.81      1895\n",
      "\n",
      "MultinomialNB()\n",
      "accuracy\n",
      "0.7968337730870713\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.79      1.00      0.88      1475\n",
      "      rumour       0.95      0.09      0.16       420\n",
      "\n",
      "    accuracy                           0.80      1895\n",
      "   macro avg       0.87      0.54      0.52      1895\n",
      "weighted avg       0.83      0.80      0.72      1895\n",
      "\n",
      "LinearSVC()\n",
      "accuracy\n",
      "0.8992084432717679\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.90      0.98      0.94      1475\n",
      "      rumour       0.91      0.61      0.73       420\n",
      "\n",
      "    accuracy                           0.90      1895\n",
      "   macro avg       0.90      0.79      0.83      1895\n",
      "weighted avg       0.90      0.90      0.89      1895\n",
      "\n",
      "LogisticRegression()\n",
      "accuracy\n",
      "0.8316622691292876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.82      1.00      0.90      1475\n",
      "      rumour       0.96      0.25      0.40       420\n",
      "\n",
      "    accuracy                           0.83      1895\n",
      "   macro avg       0.89      0.62      0.65      1895\n",
      "weighted avg       0.85      0.83      0.79      1895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def do_multiple_10foldcrossvalidation(clfs,data,classifications):\n",
    "    for clf in clfs:\n",
    "        predictions = model_selection.cross_val_predict(clf, data,classifications, cv=10)\n",
    "        print (clf)\n",
    "        print (\"accuracy\")\n",
    "        print (accuracy_score(classifications,predictions))\n",
    "        print (classification_report(classifications,predictions))\n",
    "        \n",
    "do_multiple_10foldcrossvalidation(clfs,x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc650265",
   "metadata": {
    "id": "fc650265"
   },
   "source": [
    "### Using td-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6370b51",
   "metadata": {
    "id": "e6370b51"
   },
   "outputs": [],
   "source": [
    "# need to write manually for better tokenize\n",
    "td = TfidfVectorizer(stop_words='english')\n",
    "x_train = td.fit_transform(train_merge_events)\n",
    "x_dev = td.transform(dev_merge_events)\n",
    "\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#vectorizer = CountVectorizer(stop_words='english')\n",
    "#x_train  = vectorizer.fit_transform(train_merge_events)\n",
    "#x_dev = vectorizer.transform(dev_merge_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "649f92ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "649f92ac",
    "outputId": "30136108-362a-4df7-c680-055094949524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With alpha = 0.001 the accuracy of Naive Bayes is 0.91139\n",
      "With alpha = 0.005 the accuracy of Naive Bayes is 0.91297\n",
      "With alpha = 0.01 the accuracy of Naive Bayes is 0.90190\n",
      "With alpha = 0.1 the accuracy of Naive Bayes is 0.92089\n",
      "With alpha = 0.3 the accuracy of Naive Bayes is 0.88449\n",
      "With alpha = 0.5 the accuracy of Naive Bayes is 0.85601\n",
      "With alpha = 1 the accuracy of Naive Bayes is 0.80380\n",
      "The best setting for Naive Bayes is alpha = 0.1 with accuracy = 0.92089\n"
     ]
    }
   ],
   "source": [
    "# k fold to find the optimize hyperparameter\n",
    "alphas = [0.001,0.005,0.01,0.1,0.3,0.5,1]\n",
    "max_nb = 0\n",
    "for alpha in alphas:\n",
    "    nb = MultinomialNB(alpha=alpha)\n",
    "    nb_predict = nb.fit(x_train, train_labels).predict(x_dev)    \n",
    "    nb_accuracy = accuracy_score(dev_labels,nb_predict)\n",
    "    print('With alpha = {alpha} the accuracy of Naive Bayes is {acc:.5f}'.format(alpha=alpha, acc = nb_accuracy))\n",
    "    if nb_accuracy > max_nb:\n",
    "        max_nb = nb_accuracy\n",
    "        max_alpha = alpha\n",
    "print(\"The best setting for Naive Bayes is alpha = {alpha} with accuracy = {acc:.5f}\".format(alpha=max_alpha,acc=max_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b5163",
   "metadata": {
    "id": "820b5163"
   },
   "source": [
    "### BERT\n",
    "Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfbac9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cfbac9a",
    "outputId": "4113a461-1ff8-4009-f666-b6a8d069dffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e435b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dhHyWQDopcj4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhHyWQDopcj4",
    "outputId": "21e63c3a-b7ac-40ad-9d73-15c9d921197a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading BERT model.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"Done loading BERT model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8k17okjUpskU",
   "metadata": {
    "id": "8k17okjUpskU"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#load BERT's WordPiece tokenisation model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "T4Nn_JEUsaOl",
   "metadata": {
    "id": "T4Nn_JEUsaOl"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(event[0])\n",
    "# tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "tokens = ['[CLS]'] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "P0Vh6rfctB6-",
   "metadata": {
    "id": "P0Vh6rfctB6-"
   },
   "outputs": [],
   "source": [
    "tokens=['CLS']\n",
    "for tweet in event:\n",
    "  t_tokens = tokenizer.tokenize(tweet)\n",
    "  tokens = tokens + t_tokens + ['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ykNY4Po0bER",
   "metadata": {
    "id": "7ykNY4Po0bER"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename, maxlen):\n",
    "\n",
    "        #Store the contents of the file in a pandas dataframe\n",
    "        self.df = pd.read_csv(filename, delimiter = '\\t')\n",
    "\n",
    "        #Initialize the BERT tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'sentence']\n",
    "        label = self.df.loc[index, 'label']\n",
    "\n",
    "        #Preprocessing the text to be suitable for BERT\n",
    "        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "MRdMMxDW7U_i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRdMMxDW7U_i",
    "outputId": "f3425451-178d-47f7-975d-ae99acd93c63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing training and development data.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Creating instances of training and development set\n",
    "#maxlen sets the maximum length a sentence can have\n",
    "#any sentence longer than this length is truncated to the maxlen size\n",
    "train_set = SSTDataset(filename = 'train.tsv', maxlen = 500)\n",
    "dev_set = SSTDataset(filename = 'dev.tsv', maxlen = 500)\n",
    "\n",
    "#Creating intsances of training and development dataloaders\n",
    "train_loader = DataLoader(train_set,batch_size = 4, num_workers = 0)\n",
    "dev_loader = DataLoader(dev_set, batch_size = 4, num_workers = 0)\n",
    "\n",
    "print(\"Done preprocessing training and development data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "Cxrpv1qP8oKx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cxrpv1qP8oKx",
    "outputId": "dcaccb6b-1fa1-4098-f501-1cf6e95b83c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7ff8534d0150>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "RSrj5mXU7yhz",
   "metadata": {
    "id": "RSrj5mXU7yhz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        #Classification layer\n",
    "        #input dimension is 768 because [CLS] embedding has a dimension of 768\n",
    "        #output dimension is 1 because we're working with a binary classification problem\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vZ76-l0A77Bo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZ76-l0A77Bo",
    "outputId": "8db20d76-d187-4cc2-a820-44e25dd366e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the sentiment classifier, initialised with pretrained BERT-BASE parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the sentiment classifier.\n"
     ]
    }
   ],
   "source": [
    "gpu = 0 #gpu ID\n",
    "\n",
    "print(\"Creating the sentiment classifier, initialised with pretrained BERT-BASE parameters...\")\n",
    "net = SentimentClassifier()\n",
    "net.cuda(gpu) #Enable gpu support for the model\n",
    "print(\"Done creating the sentiment classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qNNd83vr8LAL",
   "metadata": {
    "id": "qNNd83vr8LAL"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8occKO88EcH",
   "metadata": {
    "id": "c8occKO88EcH"
   },
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "def evaluate(net, criterion, dataloader, gpu):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "z2QV4Pxz8Jz7",
   "metadata": {
    "id": "z2QV4Pxz8Jz7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
    "\n",
    "    best_acc = 0\n",
    "    st = time.time()\n",
    "    for ep in range(max_eps):\n",
    "        \n",
    "        net.train()\n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "              \n",
    "            if it % 100 == 0:\n",
    "                \n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "\n",
    "        \n",
    "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
    "        if dev_acc > best_acc:\n",
    "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "            best_acc = dev_acc\n",
    "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "OcEZotJw8Smo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcEZotJw8Smo",
    "outputId": "849c42f8-6b5b-413d-a9cd-514cb8acafa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. Loss: nan; Accuracy: 0.75; Time taken (s): 0.8980262279510498\n",
      "Iteration 100 of epoch 0 complete. Loss: nan; Accuracy: 0.75; Time taken (s): 70.66284918785095\n",
      "Iteration 200 of epoch 0 complete. Loss: nan; Accuracy: 0.75; Time taken (s): 70.82386493682861\n",
      "Iteration 300 of epoch 0 complete. Loss: nan; Accuracy: 0.75; Time taken (s): 70.32176375389099\n",
      "Iteration 400 of epoch 0 complete. Loss: nan; Accuracy: 1.0; Time taken (s): 70.52647161483765\n",
      "Epoch 0 complete! Development Accuracy: 0.7684563994407654; Development Loss: nan\n",
      "Best development accuracy improved from 0 to 0.7684563994407654, saving model...\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 1\n",
    "\n",
    "#fine-tune the model\n",
    "train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50bd81",
   "metadata": {
    "id": "af50bd81"
   },
   "source": [
    "### LSTM\n",
    "model the source tweet and replies as a sequence of tweets using recurrent networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004447d4",
   "metadata": {
    "id": "004447d4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
