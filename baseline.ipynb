{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a35d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import json\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f974c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load label for train \n",
    "train_label_file = open('project-data/train.label.txt', 'r')\n",
    "train_labels = train_label_file.readlines()\n",
    "train_labels = [label.strip('\\n') for label in train_labels]\n",
    "\n",
    "#load label for dev \n",
    "dev_label_file = open('project-data/dev.label.txt', 'r')\n",
    "dev_labels = dev_label_file.readlines()\n",
    "dev_labels = [label.strip('\\n') for label in dev_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4021e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open train text file\n",
    "f = open(f'./tweet_text.pckl','rb')\n",
    "train_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# open dev text file\n",
    "f = open(f'./dev_tweet_text.pckl','rb')\n",
    "dev_data = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f8d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text) #remove @mention\n",
    "    text = re.sub(r'#','',text) # remove the hashtag symbol\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '',text) #remove hyperlink\n",
    "    text = re.sub(r'\\n','',text) # remove \\n \n",
    "    return text\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    for j in range(len(train_data[i])):\n",
    "        train_data[i][j] = clean_text(train_data[i][j])\n",
    "        \n",
    "for i in range(len(dev_data)):\n",
    "    for j in range(len(dev_data[i])):\n",
    "        dev_data[i][j] = clean_text(dev_data[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e188276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge source tweeet and reply tweet together for train data\n",
    "train_merge_events=[]\n",
    "for event in train_data:\n",
    "    merge = ''\n",
    "    for tweet in event:\n",
    "        merge = merge + tweet\n",
    "    train_merge_events.append(merge)\n",
    "    \n",
    "    \n",
    "# merge source tweeet and reply tweet together for dev data\n",
    "dev_merge_events=[]\n",
    "for event in dev_data:\n",
    "    merge = ''\n",
    "    for tweet in event:\n",
    "        merge = merge + tweet\n",
    "    dev_merge_events.append(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ecc06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweet(tweet):\n",
    "    \"\"\"Get all of the tokens in a set of tweets\"\"\"\n",
    "    twt = nltk.tokenize.TweetTokenizer()\n",
    "    # combine stop words and punctuation\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stop = stopwords + list(string.punctuation)\n",
    "    # filter out stop words and punctuation and send to lower case\n",
    "    tokens = [token.lower() for token in twt.tokenize(tweet)\n",
    "              if token.lower() not in stop]\n",
    "    tokens = [word for word in tokens if re.search('[a-zA-Z]',word) is not None] # filter out word not contain alphabet\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eedf7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweetv2(tweet):\n",
    "    \"\"\"Get all of the tokens in a set of tweets\"\"\"\n",
    "    twt = nltk.tokenize.TweetTokenizer()\n",
    "    # combine stop words and punctuation\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stop = stopwords + list(string.punctuation)\n",
    "    # create the stemmer\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    # filter out stop words and punctuation and send to lower case\n",
    "    tokens = [ stemmer.stem(token) for token in twt.tokenize(tweet)\n",
    "              if token.lower() not in stop]\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29a7d7",
   "metadata": {},
   "source": [
    "### Normal bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422ff238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of word \n",
    "def bow(data,labels):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)):\n",
    "        tokens = tokenize_tweet(data[i])\n",
    "        \n",
    "        vocab = collections.defaultdict(int)\n",
    "        for word in tokens:\n",
    "            vocab[word] += 1 \n",
    "        x.append(vocab)\n",
    "        y.append(labels[i])\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e7b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = bow(train_merge_events,train_labels)\n",
    "x_dev,y_dev = bow(dev_merge_events,dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a1b9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_dev = vectorizer.transform(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b53067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [KNeighborsClassifier(),DecisionTreeClassifier(),RandomForestClassifier(),\n",
    "        MultinomialNB(),LinearSVC(),LogisticRegression()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e45b38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier()\n",
      "accuracy\n",
      "0.8158311345646438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.81      0.99      0.89      1475\n",
      "      rumour       0.84      0.21      0.34       420\n",
      "\n",
      "    accuracy                           0.82      1895\n",
      "   macro avg       0.83      0.60      0.61      1895\n",
      "weighted avg       0.82      0.82      0.77      1895\n",
      "\n",
      "DecisionTreeClassifier()\n",
      "accuracy\n",
      "0.8379947229551451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.88      0.91      0.90      1475\n",
      "      rumour       0.65      0.58      0.61       420\n",
      "\n",
      "    accuracy                           0.84      1895\n",
      "   macro avg       0.77      0.75      0.76      1895\n",
      "weighted avg       0.83      0.84      0.83      1895\n",
      "\n",
      "RandomForestClassifier()\n",
      "accuracy\n",
      "0.8358839050131927\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.84      0.98      0.90      1475\n",
      "      rumour       0.82      0.34      0.48       420\n",
      "\n",
      "    accuracy                           0.84      1895\n",
      "   macro avg       0.83      0.66      0.69      1895\n",
      "weighted avg       0.83      0.84      0.81      1895\n",
      "\n",
      "MultinomialNB()\n",
      "accuracy\n",
      "0.8633245382585752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.94      0.88      0.91      1475\n",
      "      rumour       0.66      0.81      0.72       420\n",
      "\n",
      "    accuracy                           0.86      1895\n",
      "   macro avg       0.80      0.84      0.82      1895\n",
      "weighted avg       0.88      0.86      0.87      1895\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\envs\\CV\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC()\n",
      "accuracy\n",
      "0.8722955145118734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.90      0.93      0.92      1475\n",
      "      rumour       0.74      0.65      0.69       420\n",
      "\n",
      "    accuracy                           0.87      1895\n",
      "   macro avg       0.82      0.79      0.81      1895\n",
      "weighted avg       0.87      0.87      0.87      1895\n",
      "\n",
      "LogisticRegression()\n",
      "accuracy\n",
      "0.8949868073878628\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.90      0.97      0.93      1475\n",
      "      rumour       0.86      0.63      0.73       420\n",
      "\n",
      "    accuracy                           0.89      1895\n",
      "   macro avg       0.88      0.80      0.83      1895\n",
      "weighted avg       0.89      0.89      0.89      1895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def do_multiple_10foldcrossvalidation(clfs,data,classifications):\n",
    "    for clf in clfs:\n",
    "        predictions = model_selection.cross_val_predict(clf, data,classifications, cv=10)\n",
    "        print (clf)\n",
    "        print (\"accuracy\")\n",
    "        print (accuracy_score(classifications,predictions))\n",
    "        print (classification_report(classifications,predictions))\n",
    "        \n",
    "do_multiple_10foldcrossvalidation(clfs,x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "631e2347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With alpha = 0.001 the accuracy of Naive Bayes is 0.90032\n",
      "With alpha = 0.005 the accuracy of Naive Bayes is 0.89715\n",
      "With alpha = 0.01 the accuracy of Naive Bayes is 0.88924\n",
      "With alpha = 0.1 the accuracy of Naive Bayes is 0.88608\n",
      "With alpha = 0.3 the accuracy of Naive Bayes is 0.87816\n",
      "With alpha = 0.5 the accuracy of Naive Bayes is 0.88133\n",
      "With alpha = 1 the accuracy of Naive Bayes is 0.88608\n",
      "The best setting for Naive Bayes is alpha = 0.001 with accuracy = 0.90032\n"
     ]
    }
   ],
   "source": [
    "# k fold to find the optimize hyperparameter\n",
    "alphas = [0.001,0.005,0.01,0.1,0.3,0.5,1]\n",
    "max_nb = 0\n",
    "for alpha in alphas:\n",
    "    nb = MultinomialNB(alpha=alpha)\n",
    "    nb_predict = nb.fit(x_train, y_train).predict(x_dev)    \n",
    "    nb_accuracy = accuracy_score(y_dev,nb_predict)\n",
    "    print('With alpha = {alpha} the accuracy of Naive Bayes is {acc:.5f}'.format(alpha=alpha, acc = nb_accuracy))\n",
    "    if nb_accuracy > max_nb:\n",
    "        max_nb = nb_accuracy\n",
    "        max_alpha = alpha\n",
    "print(\"The best setting for Naive Bayes is alpha = {alpha} with accuracy = {acc:.5f}\".format(alpha=max_alpha,acc=max_nb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a6ca1",
   "metadata": {},
   "source": [
    "### Using td-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "121c3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to write manually for better tokenize\n",
    "td = TfidfVectorizer(stop_words='english')\n",
    "x_train = td.fit_transform(train_merge_events)\n",
    "x_dev = td.transform(dev_merge_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be054c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier()\n",
      "accuracy\n",
      "0.7915567282321899\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.79      1.00      0.88      1475\n",
      "      rumour       0.86      0.07      0.13       420\n",
      "\n",
      "    accuracy                           0.79      1895\n",
      "   macro avg       0.82      0.53      0.51      1895\n",
      "weighted avg       0.81      0.79      0.72      1895\n",
      "\n",
      "DecisionTreeClassifier()\n",
      "accuracy\n",
      "0.8300791556728232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.88      0.91      0.89      1475\n",
      "      rumour       0.63      0.56      0.59       420\n",
      "\n",
      "    accuracy                           0.83      1895\n",
      "   macro avg       0.76      0.73      0.74      1895\n",
      "weighted avg       0.82      0.83      0.83      1895\n",
      "\n",
      "RandomForestClassifier()\n",
      "accuracy\n",
      "0.8385224274406332\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.83      0.99      0.91      1475\n",
      "      rumour       0.92      0.30      0.45       420\n",
      "\n",
      "    accuracy                           0.84      1895\n",
      "   macro avg       0.88      0.65      0.68      1895\n",
      "weighted avg       0.85      0.84      0.80      1895\n",
      "\n",
      "MultinomialNB()\n",
      "accuracy\n",
      "0.7968337730870713\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.79      1.00      0.88      1475\n",
      "      rumour       0.95      0.09      0.16       420\n",
      "\n",
      "    accuracy                           0.80      1895\n",
      "   macro avg       0.87      0.54      0.52      1895\n",
      "weighted avg       0.83      0.80      0.72      1895\n",
      "\n",
      "LinearSVC()\n",
      "accuracy\n",
      "0.8992084432717679\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.90      0.98      0.94      1475\n",
      "      rumour       0.91      0.61      0.73       420\n",
      "\n",
      "    accuracy                           0.90      1895\n",
      "   macro avg       0.90      0.79      0.83      1895\n",
      "weighted avg       0.90      0.90      0.89      1895\n",
      "\n",
      "LogisticRegression()\n",
      "accuracy\n",
      "0.8316622691292876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.82      1.00      0.90      1475\n",
      "      rumour       0.96      0.25      0.40       420\n",
      "\n",
      "    accuracy                           0.83      1895\n",
      "   macro avg       0.89      0.62      0.65      1895\n",
      "weighted avg       0.85      0.83      0.79      1895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def do_multiple_10foldcrossvalidation(clfs,data,classifications):\n",
    "    for clf in clfs:\n",
    "        predictions = model_selection.cross_val_predict(clf, data,classifications, cv=10)\n",
    "        print (clf)\n",
    "        print (\"accuracy\")\n",
    "        print (accuracy_score(classifications,predictions))\n",
    "        print (classification_report(classifications,predictions))\n",
    "        \n",
    "do_multiple_10foldcrossvalidation(clfs,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18e43d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With alpha = 0.001 the accuracy of Naive Bayes is 0.91456\n",
      "With alpha = 0.005 the accuracy of Naive Bayes is 0.91772\n",
      "With alpha = 0.01 the accuracy of Naive Bayes is 0.90981\n",
      "With alpha = 0.1 the accuracy of Naive Bayes is 0.91772\n",
      "With alpha = 0.3 the accuracy of Naive Bayes is 0.88608\n",
      "With alpha = 0.5 the accuracy of Naive Bayes is 0.85285\n",
      "With alpha = 1 the accuracy of Naive Bayes is 0.79905\n",
      "The best setting for Naive Bayes is alpha = 0.005 with accuracy = 0.91772\n"
     ]
    }
   ],
   "source": [
    "# k fold to find the optimize hyperparameter\n",
    "alphas = [0.001,0.005,0.01,0.1,0.3,0.5,1]\n",
    "max_nb = 0\n",
    "for alpha in alphas:\n",
    "    nb = MultinomialNB(alpha=alpha)\n",
    "    nb_predict = nb.fit(x_train, y_train).predict(x_dev)    \n",
    "    nb_accuracy = accuracy_score(y_dev,nb_predict)\n",
    "    print('With alpha = {alpha} the accuracy of Naive Bayes is {acc:.5f}'.format(alpha=alpha, acc = nb_accuracy))\n",
    "    if nb_accuracy > max_nb:\n",
    "        max_nb = nb_accuracy\n",
    "        max_alpha = alpha\n",
    "print(\"The best setting for Naive Bayes is alpha = {alpha} with accuracy = {acc:.5f}\".format(alpha=max_alpha,acc=max_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fcb225",
   "metadata": {},
   "source": [
    "### Result for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c72aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data \n",
    "test_data_file = open('project-data/test.data.txt', 'r')\n",
    "test_lines = test_data_file.readlines()\n",
    "test_events =[]\n",
    "# Strips the newline character\n",
    "for line in test_lines:\n",
    "    test_events.append(list(map(int,line.strip('\\n').split(','))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "37792899",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[]\n",
    "for event in test_events:\n",
    "    tweet_list=[]\n",
    "    for tweet in event: \n",
    "        f = open('project-data/tweet-objects/'+str(tweet)+'.json')\n",
    "        data = json.load(f)\n",
    "        tweet_list.append(data['text'])\n",
    "    test_data.append(tweet_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "315572de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to pickle file\n",
    "f = open(f'./test_tweet_text.pckl','wb')\n",
    "pickle.dump(test_data,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cffeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f'./test_tweet_text.pckl','rb')\n",
    "test_data = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b040bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "    for j in range(len(test_data[i])):\n",
    "        test_data[i][j] = clean_text(test_data[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "114ebd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merge_events=[]\n",
    "for event in test_data:\n",
    "    merge = ''\n",
    "    for tweet in event:\n",
    "        merge = merge + tweet\n",
    "    test_merge_events.append(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68edbe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = TfidfVectorizer(stop_words='english')\n",
    "x_train = td.fit_transform(train_merge_events)\n",
    "x_test = td.transform(test_merge_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5723f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB(alpha= 0.005)\n",
    "nb_predict = nb.fit(x_train, y_train).predict(x_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e593c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=[]\n",
    "for result in nb_predict:\n",
    "    if result == \"nonrumour\":\n",
    "        predict.append(0)\n",
    "    else:\n",
    "        predict.append(1)\n",
    "df = pd.DataFrame({\"Id\": range(len(predict)),\"Predicted\": predict}) \n",
    "df.to_csv('nb_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed4d0404",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_predict = lr.fit(x_train, y_train).predict(x_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cda911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=[]\n",
    "for result in lr_predict:\n",
    "    if result == \"nonrumour\":\n",
    "        predict.append(0)\n",
    "    else:\n",
    "        predict.append(1)\n",
    "df = pd.DataFrame({\"Id\": range(len(predict)),\"Predicted\": predict}) \n",
    "df.to_csv('lr_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb2a443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC = LinearSVC()\n",
    "SVC_predict = SVC.fit(x_train, y_train).predict(x_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4582ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the highest score atm\n",
    "predict=[]\n",
    "for result in SVC_predict:\n",
    "    if result == \"nonrumour\":\n",
    "        predict.append(0)\n",
    "    else:\n",
    "        predict.append(1)\n",
    "df = pd.DataFrame({\"Id\": range(len(predict)),\"Predicted\": predict}) \n",
    "df.to_csv('svc_test.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
